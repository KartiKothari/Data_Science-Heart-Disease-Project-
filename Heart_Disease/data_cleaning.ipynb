{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0263c3-ca96-4648-a1bc-4631b404879e",
   "metadata": {},
   "source": [
    "##### Prediciting Heart-Disease by ML Algorithm\n",
    "Steps:\n",
    "1.Problem Statement \n",
    "2.Collecting data\n",
    "3.Evalution\n",
    "4.Features\n",
    "5.Modeling\n",
    "6.Experimentaion\n",
    "\n",
    "1. Problem Defination\n",
    "Given clinical parameters, can we predict if someone have heart disease or not using ML\n",
    "2. Data\n",
    "from uci heart disease dataset - kaggle\n",
    "https://archive.ics.uci.edu/dataset/45/heart+disease\n",
    "3. Feature\n",
    "    id (Unique id for each patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c748916-2615-4662-9b00-9e2428891622",
   "metadata": {},
   "outputs": [],
   "source": [
    "age (Age of the patient in years)    \n",
    "origin (place of study    )\n",
    "sex (Male/Femal    e)\n",
    "cp chest pain type ([typical angina, atypical angina, non-anginal, asymptomati    c])\n",
    "trestbps resting blood pressure (resting blood pressure (in mm Hg on admission to the hospit    al))\n",
    "chol (serum cholesterol in m    g/dl)\n",
    "fbs (if fasting blood sugar > 120     mg/dl)\n",
    "restecg (resting electrocardiographic r    esults)\n",
    "-- Values: [normal, stt abnormality, lv hype    rtrophy]\n",
    "thalach: maximum heart rate     achieved\n",
    "exang: exercise-induced angina (Tr    ue/ False)\n",
    "oldpeak: ST depression induced by exercise relat    ive to rest\n",
    "slope: the slope of the peak exercis    e ST segment\n",
    "ca: number of major vessels (0-3) colored b    y fluoroscopy\n",
    "thal: [normal; fixed defect; reve    rsible defect]\n",
    "num: the predicted attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8fbd45-7a5f-4a29-bd8f-21899e1d87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cted attribute\n",
    "\n",
    "#Importing Libaries\n",
    "\n",
    "#For EDA (exploratory data analysis)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Models from Scikit-Learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Model Evaluations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "df = pd.read_csv(\"007 heart-disease.csv\")\n",
    "df.head()\n",
    "\n",
    "df.tail()\n",
    "\n",
    "df.target.value_counts()\n",
    "\n",
    "df.sex.value_counts()\n",
    "\n",
    "pd.crosstab(df.target,df.sex)\n",
    "\n",
    "pd.crosstab(df.target,df.sex).plot(kind='bar',figsize=(10,6),color=['salmon','lightblue'])\n",
    "plt.title(\"Heart Disease Freq\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.legend([\"Female\",\"Male\"])\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "df.head(2)\n",
    "\n",
    "#Graph between age, thalach and target\n",
    "plt.figure(figsize=(10,6))\n",
    "#for target=1\n",
    "plt.scatter(df.age[df.target==1],df.thalach[df.target==1], color='salmon')\n",
    "#for target=0\n",
    "plt.scatter(df.age[df.target==0],df.thalach[df.target==0], color='lightblue')\n",
    "#adding labels\n",
    "plt.title(\"Max Heart Rate vs Age\")\n",
    "plt.ylabel(\"Max Heart Rate\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.legend([\"Disease\",\"Healthy\"]);\n",
    "\n",
    "\n",
    "#Conclude Young have more max heart rate, Index of Disease line is more\n",
    "\n",
    "\n",
    "\n",
    "#Chest pain(cp) vs target\n",
    "pd.crosstab(df.cp,df.target).plot(kind='bar',figsize=(10,6),color=[\"salmon\",\"lightblue\"]);\n",
    "\n",
    "\n",
    "# 1 and 2 have more Disease patient proposition type\n",
    "\n",
    "df.age.plot.hist();\n",
    "plt.xlabel(\"Age\")\n",
    "\n",
    "##Remaining \n",
    "#plt.scatter(df.age,df.age.value_counts,color='red')\n",
    "\n",
    "age = df.age.unique()\n",
    "\n",
    "value = df.age.value_counts()\n",
    "\n",
    "plt.scatter(age,value,color='red')\n",
    "\n",
    "df.corr()\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.heatmap(corr_matrix,\n",
    "                 annot=True,\n",
    "                 linewidths=0.5,\n",
    "                 fmt=\"0.2f\",\n",
    "                 cmap=\"YlGnBu\");\n",
    "# bottom, top = ax.get_ylim()\n",
    "# ax.set_ylim(bottom =+0.5,top=-0.5)\n",
    "\n",
    "\n",
    "#Correlation between them, if its zero or near it then target value not depend on that variable such as fbs \n",
    "\n",
    "#Modelling\n",
    "X = df.drop([\"target\"],axis=1)\n",
    "y=df.target.values\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# X = X.rename(columns={' ': ''})\n",
    "# y = y.rename({' ': ''})\n",
    "\n",
    "X_train\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "X_test = sc_x.transform(X_test)\n",
    "\n",
    "\n",
    "log = LogisticRegression()\n",
    "log.fit(X_train,y_train)\n",
    "y_pred = log.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "model_score =log.score(X_test,y_test)\n",
    "\n",
    "model_score\n",
    "\n",
    "# models = {\"LogisticRegression\":LogisticRegression,\"KNN\":KNeighborsClassifier,\"RandomForest\":RandomForestClassifier}\n",
    "\n",
    "# def models_selection(models,X_train,X_test,y_train,y_test):\n",
    "#     models_score = {}\n",
    "#     np.random.seed(42)\n",
    "#     for name, model in models.items():\n",
    "#         print(name)\n",
    "#         print(model)\n",
    "#         model().fit(X_train,y_train)\n",
    "#         model_score = model().score(X_test,y_test)\n",
    "#     return models_score\n",
    "\n",
    "# models_scores = models_selection(models=models,X_train=X_train,X_test=X_test,y_train=y_train,y_test=y_test)\n",
    "\n",
    "#Hyperparameters\n",
    "log_reg_grid = {\"C\": np.logspace(-4,4,20),\"solver\":[\"liblinear\"]}\n",
    "\n",
    "rs_log_reg = RandomizedSearchCV(LogisticRegression(),param_distributions = log_reg_grid,cv=5,n_iter=20,verbose=True)\n",
    "rs_log_reg.fit(X_train,y_train)\n",
    "\n",
    "rs_log_reg.best_params_\n",
    "\n",
    "rs_log_reg.score(X_test,y_test)\n",
    "\n",
    "## Got for hyperparamters for Log\n",
    "\n",
    "# Now for RandomForest\n",
    "rs_grid = {\"n_estimators\":np.arange(10,1000,50),\n",
    "           \"max_depth\":[None,3,5,20],\n",
    "           \"min_samples_split\":np.arange(2,20,2),\n",
    "           \"min_samples_leaf\": np.arange(1,20,2)}\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=200,criterion='entropy',random_state=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "model_score =log.score(X_test,y_test)\n",
    "\n",
    "model_score\n",
    "\n",
    "np.random.seed(42)\n",
    "rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n",
    "                           param_distributions=rs_grid,\n",
    "                           cv=5,\n",
    "                           n_iter=25,\n",
    "                           verbose=True)\n",
    "rs_rf.fit(X_train,y_train)\n",
    "\n",
    "rs_rf.best_params_\n",
    "\n",
    "rs_rf.score(X_test,y_test)\n",
    "\n",
    "## GridSearchCV\n",
    "\n",
    "log_grid = {\"C\":np.logspace(-4,4,30),\"solver\":[\"liblinear\"]}\n",
    "\n",
    "gs_grid = GridSearchCV(LogisticRegression(),param_grid=log_grid,cv=5,verbose=True)\n",
    "\n",
    "gs_grid.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "gs_grid.best_params_\n",
    "\n",
    "gs_grid.score(X_test,y_test)\n",
    "\n",
    "y_pred = gs_grid.predict(X_test)\n",
    "\n",
    "y_pred\n",
    "\n",
    "##Making ROC Curve\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "display=metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name= 'gs_grid')\n",
    "display.plot();\n",
    "\n",
    "confusion_matrix(y_test,y_pred)\n",
    "\n",
    "#plot seaborn\n",
    "sns.set(font_scale=1.5)\n",
    "def plot_conf_mat(y_test,y_pred):\n",
    "    fig, ax = plt.subplots(figsize=(3,3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test,y_pred), annot=True, cbar=False)\n",
    "    plt.xlabel(\"y_pred\")\n",
    "    plt.ylabel(\"y_test\")\n",
    "plot_conf_mat(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "X=sc_x.transform(X)\n",
    "\n",
    "clf = LogisticRegression(C= 0.01610262027560939, solver= 'liblinear')\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "clf.score(X,y)\n",
    "\n",
    "#Cross-validation accuracy\n",
    "cv_acc = cross_val_score(clf,X,y,cv=5,scoring='accuracy')\n",
    "cv_acc\n",
    "\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "#Cross-validation Precision\n",
    "cv_pre = cross_val_score(clf,X,y,cv=5,scoring=\"precision\")\n",
    "cv_pre = np.mean(cv_pre)\n",
    "cv_pre\n",
    "\n",
    "#Cross-validation Recall\n",
    "cv_recall = cross_val_score(clf,X,y,cv=5,scoring=\"recall\")\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "#Cross-vali  f1_score\n",
    "cv_f1 = cross_val_score(clf,X,y,cv=5,scoring=\"f1\")\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "#visualize scoresmetrics\n",
    "cv_metrics = pd.DataFrame({\"Accuracy\":cv_acc,\"Precision\":cv_pre,\"Recall\":cv_recall,\"f1_score\":cv_f1},index=[0])\n",
    "\n",
    "cv_metrics\n",
    "\n",
    "cv_metrics.T.plot.bar(title=\"Plot graph for Cross_validation\",legend=False)\n",
    "\n",
    "type(clf.coef_)\n",
    "\n",
    "type(clf.coef_[0])\n",
    "\n",
    "feature_dict = dict(zip(df.columns,clf.coef_[0]))\n",
    "feature_dict\n",
    "\n",
    "feature_df = pd.DataFrame(feature_dict,index=[0])\n",
    "\n",
    "feature_df.T.plot.bar(title=\"Features Importance\",legend=False)\n",
    "\n",
    "#### Pickle and Joblib For saving File\n",
    "\n",
    "import pickle\n",
    "#save an existing model to file\n",
    "pickle.dump(gs_grid, open(\"save_gs_grid_model.pkl\",\"wb\"))\n",
    "\n",
    "#Load the pickle file\n",
    "Load_pickle = pickle.load(open(\"save_gs_grid_model.pkl\",\"rb\"))\n",
    "\n",
    "#Use it\n",
    "y_pred_pickle = Load_pickle.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test,y_pred_pickle)\n",
    "\n",
    "confusion_matrix(y_test,y_pred)\n",
    "\n",
    "from joblib import dump,load\n",
    "# Save file\n",
    "dump(gs_grid, filename = \"save_gs_grid_model.joblib\")\n",
    "\n",
    "loaded_joblib = load(filename = \"save_gs_grid_model.joblib\")\n",
    "\n",
    "y_pred_joblib = loaded_joblib.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test,y_pred_joblib)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
